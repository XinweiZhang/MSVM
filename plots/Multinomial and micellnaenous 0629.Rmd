---
title: "Multinomial and micellnaenous 0629"
author: "Xinwei"
date: "6/29/2020"
output: html_document
---

```{r setup, include=FALSE}
library(CVXR)
library(MASS)
library(R.matlab)
library(e1071)
library(glmnet)
rm(list = ls())

L2norm <- function(x){
  sqrt(sum(x^2))
}
L2normsquare <- function(x){
  sum(x^2)
}
draw_line <- function(beta, lty = 2){
  abline(a = -beta[3]/beta[2], b = -beta[1]/beta[2], lty =lty, col = "red")
}
Bayes_rule_Gaussian <- function(mu1,mu2){
  c((mu1-mu2), 1/2*sum(mu2^2)-1/2*sum(mu1^2))
}

max_of_all <- function(w){
  max(L2normsquare(w[2,] - w[1,]), L2normsquare(w[3,] - w[1,]), L2normsquare(w[4,] - w[1,]),
      L2normsquare(w[3,] - w[2,]), L2normsquare(w[4,] - w[2,]), 
      L2normsquare(w[4,] - w[3,]))
}
sum_of_max <- function(w){
  sum(max(L2normsquare(w[2,] - w[1,]), L2normsquare(w[3,] - w[1,]), L2normsquare(w[4,] - w[1,])), 
      max(L2normsquare(w[1,] - w[2,]),L2normsquare(w[3,] - w[2,]), L2normsquare(w[4,] - w[2,])),
      max(L2normsquare(w[1,] - w[3,]), L2normsquare(w[2,] - w[3,]), L2normsquare(w[4,] - w[3,])), 
      max(L2normsquare(w[4,] - w[1,]), L2normsquare(w[4,] - w[2,]), L2normsquare(w[4,] - w[3,])))
}
max_of_sum <- function(w){
  max(L2normsquare(w[2,] - w[1,]) + L2normsquare(w[3,] - w[1,]) + L2normsquare(w[4,] - w[1,]),
      L2normsquare(w[1,] - w[2,]) + L2normsquare(w[3,] - w[2,]) + L2normsquare(w[4,] - w[2,]),
      L2normsquare(w[1,] - w[3,]) + L2normsquare(w[2,] - w[3,]) + L2normsquare(w[4,] - w[3,]), 
      L2normsquare(w[4,] - w[1,]) + L2normsquare(w[4,] - w[2,]) + L2normsquare(w[4,] - w[3,]))
}
Duchi <- function(w){
  L2normsquare(w)
}
New1 <- function(w){
  sum(L2normsquare(w[1,] - w[4,]), L2normsquare(w[2,] - w[4,]), L2normsquare(w[3,] - w[4,]))
}

setwd("~/Desktop/Multiclass Classification/MSVM Code")
source("~/Desktop/Multiclass Classification/MSVM Code/MSVM_weak_hard.R")
set.seed(1233)

```

# Multinomial 
## Case 1

```{r fig.width=8}
mu1 <- c(-2,1)
mu2 <- c(-2,-1)
mu3 <- c(2,1)

X1 <- matrix(mvrnorm(20,mu1,diag(0.0001,2)), nrow =20)
X2 <- matrix(mvrnorm(20,mu2,diag(0.0001,2)), nrow =20)
X3 <- matrix(mvrnorm(20,mu3,diag(0.0001,2)), nrow =20)
X <- rbind(X1,X2,X3)

C=1
y <- c(rep(1,nrow(X1)),rep(2,nrow(X2)),rep(3,nrow(X3)))
# plot(X1, col='red', xlim = c(-3,3), ylim=c(-3,3), xlab = "X1", ylab = "X2", main = "Oracle")
# 
# points(X2, col='green')
# 
# points(X3, col='blue')

multi_fit <- glmnet(x=X, y=y, family = "multinomial", alpha = 0, lambda = 1e-10)
beta <- cbind(t(do.call(cbind,multi_fit$beta)), multi_fit$a0)
L2square <- function(x){
  sum(x^2)
}
par(mfrow=c(1,2))
colSums(beta)
# beta <- beta/(L2square(beta[1,1:2])+L2square(beta[2,1:2])+L2square(beta[3,1:2]))
rescale_par <- Rescale3_beta(X,y,beta,type = "Duchi")
rescale_par$value
rescale_par$gamma
beta_rescale <- rescale_par$gamma*beta
# 

plot_decision_boundary(X, y, beta_rescale, paste("Multinomial, Duchi:", round(Duchi(beta_rescale[,1:2]),3)))
weak_hard_beta <- MSVM_Weak_Hard_opt(X,y,"Duchi")
plot_decision_boundary(X, y, weak_hard_beta, paste("MSVM, Duchi:", round(Duchi(weak_hard_beta[,1:2]),3)))
draw_line(beta = beta[1,]-beta[2,])
draw_line(beta = beta[1,]-beta[3,])
draw_line(beta = beta[2,]-beta[3,])
```

## Case 2
```{r fig.width=8}
mu1 <- c(-3,5)
mu2 <- c(2,-2)
mu3 <- c(8,4)
mu4 <- c(-4,-1)
v = 0.0001
X1 <- matrix(mvrnorm(20, mu1, diag(v,2)), nrow =20)
X2 <- matrix(mvrnorm(20, mu2, diag(v,2)), nrow =20)
X3 <- matrix(mvrnorm(20, mu3, diag(v,2)), nrow =20)
X4 <- matrix(mvrnorm(20, mu4, diag(v,2)), nrow =20)
X <- rbind(X1,X2,X3,X4)

C=1
y <- c(rep(1,nrow(X1)), rep(2,nrow(X2)), rep(3,nrow(X3)), rep(4,nrow(X4)))
par(mfrow=c(1,2))
multi_fit <- glmnet(x=X, y=y, family = "multinomial", alpha = 0, lambda = 1e-10)
beta <- cbind(t(do.call(cbind,multi_fit$beta)), multi_fit$a0)
L2square <- function(x){
  sum(x^2)
}
# beta <- beta/(L2square(beta[1,1:2])+L2square(beta[2,1:2])+L2square(beta[3,1:2])+L2square(beta[4,1:2]))

colSums(beta)
rescale_par <- Rescale4_beta(X,y,beta,type = "Duchi")
rescale_par$value
rescale_par$gamma
beta_rescale <- rescale_par$gamma*beta
plot_decision_boundary(X, y, beta_rescale, paste("Multinomial, Duchi:", round(Duchi(beta_rescale[,1:2]),3)))
weak_hard_beta <- MSVM_Weak_Hard_opt4(X,y,"Duchi")
plot_decision_boundary(X, y, weak_hard_beta, paste("MSVM, Duchi:", round(Duchi(weak_hard_beta[,1:2]),3)))
draw_line(beta = beta[1,]-beta[2,])
draw_line(beta = beta[1,]-beta[3,])
draw_line(beta = beta[1,]-beta[4,])
draw_line(beta = beta[2,]-beta[3,])
draw_line(beta = beta[2,]-beta[4,])
draw_line(beta = beta[3,]-beta[4,])
```

# New proposal
In the paper New Support Vector Algorithms, Sch\Â¨olkopf, Smola, Williamson and Bartlett,  they consider the minimization of
\[
\tau(\mathbf{w}, \boldsymbol{\xi}, \rho)=\frac{1}{2}\|\mathbf{w}\|^{2}-v \rho+\frac{1}{\ell} \sum_{i} \xi_{i}
\]
subject to 
\[
\begin{array}{l}
y_{i} \cdot\left(\left(\mathbf{x}_{i} \cdot \mathbf{w}\right)+b\right) \geq \rho-\xi_{i} \\
\xi_{i} \geq 0, \quad \rho \geq 0
\end{array}
\]

What happend to generalize this idea to multiclass SVM? Consider the minimization of
\[
\tau(\mathbf{w}, \boldsymbol{\xi}, \rho)=\frac{1}{2}\sum_{i=1}^m\|\mathbf{w_i}\|^{2}-v \sum_{1\leq i < j \leq m} \rho_{ij}+ \sum_{i=1}^n \sum_{j=1}^m \xi_{ij}
\]
subject to 
\[
\begin{array}{l}
\mathbf{x}_{i}^T \mathbf{w}_{y_i} - \mathbf{x}_{i}^T \mathbf{w}_k+b_{y_i} - b_k \geq \rho_{y_ik}-\xi_{ik} &i\in[n],k\neq y_i, k\in[m]\\
\xi_{ik} \geq 0, & i\in[n],k\neq y_i,k\in[m]\\
\rho_{ij} \geq 0, &i\neq j, i\in[m], j\in[m].
\end{array}
\]


```{r echo=FALSE, fig.width=8}

X <- rbind(c(-3,5),c(2,-2),c(8,4),c(-4,-1))
y = c(1,2,3,4)
par(mfrow=c(1,2))

plot_nearest_neighbour_decision_boundary(X,y, title = "NN")
draw_line(Bayes_rule_Gaussian(X[1,,drop= F], X[2,,drop=F]))
draw_line(Bayes_rule_Gaussian(X[1,,drop= F], X[3,,drop=F]))
draw_line(Bayes_rule_Gaussian(X[1,,drop= F], X[4,,drop=F]))
draw_line(Bayes_rule_Gaussian(X[2,,drop= F], X[3,,drop=F]))
draw_line(Bayes_rule_Gaussian(X[2,,drop= F], X[4,,drop=F]))
draw_line(Bayes_rule_Gaussian(X[3,,drop= F], X[4,,drop=F]))

A <- rbind(c(1,-1,0,0),
           c(1,0,-1,0),
           c(1,0,0,-1),
           c(0,1,-1,0),
           c(0,1,0,-1),
           c(0,0,1,-1))

dif_beta <- rbind(Bayes_rule_Gaussian(X[1,,drop= F], X[2,,drop=F]),
                  Bayes_rule_Gaussian(X[1,,drop= F], X[3,,drop=F]),
                  Bayes_rule_Gaussian(X[1,,drop= F], X[4,,drop=F]),
                  Bayes_rule_Gaussian(X[2,,drop= F], X[3,,drop=F]),
                  Bayes_rule_Gaussian(X[2,,drop= F], X[4,,drop=F]),
                  Bayes_rule_Gaussian(X[3,,drop= F], X[4,,drop=F]))

NN_beta <- ginv(A)%*%dif_beta


weak_hard_beta <- MSVM_New_Weak_Hard_opt4(X,y,type = "Duchi", gamma = 1)
# weak_hard_beta/sqrt(sum(weak_hard_beta^2))
plot_decision_boundary(X,y,beta = weak_hard_beta, title = paste("New MSVM:", round(Duchi(weak_hard_beta[,1:2]),3)))
draw_line(Bayes_rule_Gaussian(X[1,,drop= F], X[2,,drop=F]))
draw_line(Bayes_rule_Gaussian(X[1,,drop= F], X[3,,drop=F]))
draw_line(Bayes_rule_Gaussian(X[1,,drop= F], X[4,,drop=F]))
draw_line(Bayes_rule_Gaussian(X[2,,drop= F], X[3,,drop=F]))
draw_line(Bayes_rule_Gaussian(X[2,,drop= F], X[4,,drop=F]))
draw_line(Bayes_rule_Gaussian(X[3,,drop= F], X[4,,drop=F]))

X = rbind(c(-2,1), c(-2,-1), c(2,1))
y = c(1,2,3)
C = 1
weak_hard_beta <-  MSVM_New_Weak_Hard_opt(X,y,type = "Duchi", gamma = 1)
plot_decision_boundary(X,y,beta = weak_hard_beta, title = paste("New MSVM:", round(Duchi(weak_hard_beta[,1:2]),3)))

# draw_line(Bayes_rule_Gaussian(X[1,,drop= F], X[2,,drop=F]))
# draw_line(Bayes_rule_Gaussian(X[1,,drop= F], X[3,,drop=F]))
# draw_line(Bayes_rule_Gaussian(X[2,,drop= F], X[3,,drop=F]))


X = rbind(c(0,2),c(0,1),c(1,0),c(2,0))
y = c(1,2,2,3)
weak_hard_beta <-  MSVM_New_Weak_Hard_opt(X,y,type = "Duchi", gamma = 1)
plot_decision_boundary(X,y,beta = weak_hard_beta, title = paste("New MSVM:", round(Duchi(weak_hard_beta[,1:2]),3)))


```
